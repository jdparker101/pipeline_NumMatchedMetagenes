##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Jacob Parker
:Release: $Id$
:Date: |today|
:Tags: Python

This pipeline calculates the number of mismatches per base for a given set of genes for mapped RNA-seq data using pysam.
The data are merged into one final table.

Overview
========

files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_mismatch.py config

Input files
-----------

Set of genes, as specified in the `pipeline.ini` file (job:gene_list).

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
from ruffus.combinatorics import product
import sys
import os
import sqlite3
from CGAT import Experiment as E
from CGAT import GTF
import CGATPipelines.Pipeline as P
from CGAT import IOTools
import pandas as pd
import csv
import re
# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

PARAMS["pipelinedir"] = os.path.dirname(__file__)


# ---------------------------------------------------
# Specific pipeline tasks
#Files must be in the format: variable1(e.g.Tissue)-ChiporControl-variable2
#(e.g.Experimentalcondition)-Furthercondition(if needed e.g. protein)
#and/orreplicate.bam
#Example: Cerebellum-Chip-minusCPT-Top1_2.bam
#Controls must have 1 as the value in the final position
#-P %(base)s%%s

@follows(mkdir("filtered_bams.dir"))
@transform("*.bam", regex(r"(.+).bam"),
          r"filtered_bams.dir/\1.filtered.bam")
def filterreads(infile,outfile):
    statement='''samtools view -b -o %(outfile)s -F 268 -q 30  %(infile)s'''
    job_memory="4G"
    P.run()


@follows(mkdir("deduplicated.dir"))
@transform(filterreads,
           regex(r"filtered_bams.dir/(.+).bam"),
           r"deduplicated.dir/\1.deduplicated.bam")
def removeduplicates(infile, outfile):
    temp_file=P.snip(outfile, ".deduplicated.bam") + ".temp.bam"
    metrics_file=P.snip(outfile, ".bam") + ".metrics"
    statement='''MarkDuplicates I=%(infile)s
                                TMP_DIR=/fastdata/mbp15jdp/tmp  
                                O=%(temp_file)s 
                                M=%(metrics_file)s > %(temp_file)s.log;
                                checkpoint;
                                samtools view 
                                -F 1024
                                -b
                                %(temp_file)s
                                > %(outfile)s;
                                checkpoint;
                                rm -r %(temp_file)s;
                                checkpoint;
                                samtools index %(outfile)s'''
    job_memory="15G"
    P.run()
      
@transform(PARAMS["job_annotations"],
           formatter(),
           "contigs.tsv")
def get_contigs(infile, outfile):
    '''Generate a pseudo-contigs file from the geneset, where the length of 
    each contigs is determined by the GTF entry with the highest end coordinate.
    Will not stop things going off the end on contigs, but that doesn't really
    matter for our purposes'''

    last_contig = None
    max_end = 0
    outlines = []
    for entry in GTF.iterator(IOTools.openFile(infile)):
   
        if last_contig and entry.contig != last_contig:
            outlines.append([entry.contig, str(max_end)])
            max_end = 0
            
        max_end = max(max_end, entry.end)
        last_contig = entry.contig

    outlines.append([last_contig, str(max_end)])
    IOTools.writeLines(outfile, outlines, header=None)


@follows("get_contigs")
@follows(mkdir("split_gtf.dir"))
@subdivide("*.categories.tsv",
	   regex(r"(.+).categories.tsv"),
           add_inputs(PARAMS["job_annotations"]),
           r"split_gtf.dir/\1_*.gtf.gz",
           r"split_gtf.dir/\1")
def split_gtf_by_category(infiles, outfiles, catname):

    catfile, gtffile = infiles
    categories = pd.read_csv(catfile, index_col=0, squeeze=True, sep="\t")
    
    # create output filepool
    outpool = IOTools.FilePool("{}_%s.gtf.gz".format(catname), force=True)

    gtffile = IOTools.openFile(gtffile)

    for gtfline in GTF.iterator(gtffile):

        try:
            transcript_id = gtfline.transcript_id
        except AttributeError:
            transcript_id = None

        try:
            gene_id = gtfline.gene_id
        except AttributeError:
            gene_id = None

        if transcript_id in categories.index:
            outpool.write(categories[transcript_id], str(gtfline) + "\n")
        elif gene_id in categories.index:
            outpool.write(categories[gene_id], str(gtfline) + "\n")

    outpool.close()

@follows(mkdir("filtered_split_gtf.dir"))
@transform(split_gtf_by_category,
           regex(r"split_gtf.dir/(.+).gtf.gz"),
           add_inputs(get_contigs),
           r"filtered_split_gtf.dir/\1.filtered.gtf.gz")
def filter_geneset(infiles, outfile):
    geneset, genome_file = infiles
    filter_extension_up=["job_extension_up"]
    filter_extension_down=PARAMS["job_extension_down"]
        
    statement = '''cgat gtf2gtf --method=merge-transcripts -I %(geneset)s
                   | cgat gff2bed --is-gtf -L %(outfile)s.log
                   | bedtools slop -l %(filter_extension_up)s -r %(filter_extension_down)s
                     -s -i - -g %(genome_file)s
                   | sort -k1,1 -k2,2n
                   | bedtools merge -c 4 -o count
                   | awk '$4>1'
                   | bedtools intersect -v -a %(geneset)s -b -
                   | bgzip > %(outfile)s'''
    job_memory="10G"
    P.run()

@follows("filter_geneset")
@mkdir("merged_filtered_split_gtf.dir")
@transform(filter_geneset,
           regex(r"filtered_split_gtf.dir/(.+).filtered.gtf.gz"),
           r"merged_filtered_split_gtf.dir/\1.merged.gtf.gz")
def MergeGTF(infile,outfile):
    tempfile=P.snip(outfile,".gz")
    statement = '''zcat %(infile)s |
                   python ~/devel/cgat/CGAT/scripts/gtf2gtf.py --method=merge-transcripts |
                   python ~/devel/cgat/CGAT/scripts/gtf2gtf.py --method=set-transcript-to-gene -S %(tempfile)s;
                   checkpoint;
                   gzip %(tempfile)s'''
    job_memory="10G"
    P.run()       

@follows("MergeGTF")
@mkdir("NumMatchedgtf.dir")
@transform(MergeGTF,
           regex(r"merged_filtered_split_gtf.dir/(.+).merged.gtf.gz"),
           r"NumMatchedgtf.dir/\1.NumMatched.gtf.gz")
def NumMatchgtfs(infile, outfile):
    tempfile=P.snip(outfile,".gz")
    if "Non" not in infile and "non" not in infile:
        intowrite=IOTools.openFile(infile)
        tempwrite=open(tempfile,"w")
        tempwrite.writelines(intowrite)
        tempwrite.close()
    if "Non" in infile or "non" in infile:
        matchedfile = "merged_filtered_split_gtf.dir/"+[i for i in os.listdir("merged_filtered_split_gtf.dir")
                       if "log" not in i and i not in infile][0]
        def file_len(fname):
            with IOTools.openFile(fname) as f:
                for i, l in enumerate(f):
                    pass
            return i + 1

        targetlines = file_len(matchedfile)
        pandasin = pd.read_csv(infile,compression="gzip",sep="\t",header=None)
        pandasinsample = pandasin.sample(targetlines)
        pandasinsample.to_csv(tempfile, header=None, index=None, sep='\t', quoting=csv.QUOTE_NONE)
    statement = ''' gzip %(tempfile)s'''
    P.run()  


@follows("NumMatchgtfs")
@follows(mkdir("geneprofiles.dir"))
@product(removeduplicates,
         formatter(".+/(?P<TRACK>.+).bam"),
         filter_geneset,
         formatter("NumMatchedgtf.dir/(?P<BIOTYPE>.+).gtf.gz"),
         "geneprofiles.dir/{TRACK[0][0]}.{BIOTYPE[1][0]}.geneprofiles.tsv",
         "{TRACK[0][0]}",
         "{BIOTYPE[1][0]}")
def splitgeneprofiles(infiles, outfile, TRACK, BIOTYPE):
    bamfile, gtffile = infiles
    base=re.search(r"(geneprofiles.dir/.+).geneprofiles.tsv", outfile, flags = 0)  
    base=base.group(1)
    if PARAMS['job_outputallprofiles']==1:
        outputprofiles="--output-all-profiles"
    elif PARAMS['job_outputallprofiles']==0:
        outputprofiles=""
    if PARAMS['job_mergepairs']==1:
        mergepairs="--merge-pairs"
    elif PARAMS['job_mergepairs']==0:
        mergepairs=""    
    statement='''python ~/devel/cgat/CGAT/scripts/bam2geneprofile.py
                 -b %(bamfile)s
                 -g %(gtffile)s
                 --reporter=gene
                 -m geneprofile
                 %(outputprofiles)s                 
                 %(mergepairs)s
                 -P %(base)s > 
                 %(outfile)s'''
    job_memory="6G"
    P.run()


@follows("NumMatchgtfs")
@follows(mkdir("tssprofiles.dir"))
@product(removeduplicates,
         formatter(".+/(?P<TRACK>.+).bam"),
         filter_geneset,
         formatter("NumMatchedgtf.dir.dir/(?P<BIOTYPE>.+).gtf.gz"),
         "tssprofiles.dir/{TRACK[0][0]}.{BIOTYPE[1][0]}.tssprofiles.tsv",
         "{TRACK[0][0]}",
         "{BIOTYPE[1][0]}")
def splittssprofiles(infiles, outfile, TRACK, BIOTYPE):
    bamfile, gtffile = infiles
    base=re.search(r"(tssprofiles.dir/.+).tssprofiles.tsv", outfile, flags = 0)  
    base=base.group(1)
    if PARAMS['job_outputallprofiles']==1:
        outputprofiles="--output-all-profiles"
    elif PARAMS['job_outputallprofiles']==0:
        outputprofiles=""
    if PARAMS['job_mergepairs']==1:
        mergepairs="--merge-pairs"
    elif PARAMS['job_mergepairs']==0:
        mergepairs=""    
    statement='''python ~/devel/cgat/CGAT/scripts/bam2geneprofile.py
                 -b %(bamfile)s
                 -g %(gtffile)s
                 --reporter=gene
                 -m tssprofile
                 %(outputprofiles)s                 
                 %(mergepairs)s
                 -P %(base)s > 
                 %(outfile)s'''
    job_memory="6G"
    P.run()

#@follows("geneprofiles")
#@transform("geneprofiles.dir/*geneprofile.profiles.tsv.gz", regex(r"(.+).geneprofile.profiles.tsv.gz"),r"\1.normalisedprofile.tsv.gz")
#def normaliseprofiles(infile, outfile):
#    statement = '''python ~/devel/pipeline_peaksandprofiles/pipeline_peaksandprofiles/normalise_profiles.py
#                -m %(infile)s
#                -L /dev/null
#                > %(outfile)s'''
#    job_memory="20G"
#    P.run()


@follows(splitgeneprofiles)
@merge("geneprofiles.dir/*_*_*.bwa*matrix.tsv.gz", "combined_geneprofiles_matrix.txt")
def mergegeneprofiles(infiles, outfile):
    infiles = " ".join(infiles)
    statement = '''python ~/devel/cgat/CGAT/scripts/combine_tables.py 
                   --regex-filename="geneprofiles.dir/(.+)_(.+)_(.+).bwa.filtered.deduplicated.(.+).filtered.matrix.tsv.gz" 
                   --cat pulldown,condition,replicate,category 
                   -S %(outfile)s
                   %(infiles)s'''
    job_memory="10G" 
    P.run()

@follows(splittssprofiles)
@merge("tssprofiles.dir/*_*_*.bwa*matrix.tsv.gz", "combined_tssprofiles_matrix.txt")
def mergetssprofiles(infiles, outfile):
    infiles = " ".join(infiles)
    statement = '''python ~/devel/cgat/CGAT/scripts/combine_tables.py 
                   --regex-filename="tssprofiles.dir/(.+)_(.+)_(.+).bwa.filtered.deduplicated.(.+).filtered.matrix.tsv.gz" 
                   --cat pulldown,condition,replicate,category 
                   -S %(outfile)s'''

    job_memory="10G"
    P.run()
# ---------------------------------------------------
# Generic pipeline tasks
@follows(splitgeneprofiles,splittssprofiles,mergegeneprofiles,mergetssprofiles)
def full():
    pass


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))

